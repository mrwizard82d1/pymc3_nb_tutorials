{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# General API quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format='retina'\n",
    "az.style.use('arviz-darkgrid')\n",
    "print(f'Running on PyMC3 v{pm.__version__}')\n",
    "print(f'Running on ArviZ v{az.__version__}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model creation\n",
    "\n",
    "Models in PyMC3 are centered around the `Model` class. It has references to all random variables (RVs) and computes the model logp and its gradients. Usually, you would instantiate it as part of a `with` context:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # Model definition\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We discuss RVs further below but let's create a simple model to explore the `Model` class."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    mu = pm.Normal('mu', mu=0, sigma=1)\n",
    "    obs = pm.Normal('obs', mu=mu, sigma=1, observed=np.random.randn(100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model.basic_RVs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.free_RVs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.observed_RVs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.logp({'mu': 0})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It's worth highlighting the design choice we made with logp. As you can see above, `logp` is being called with arguments, so it's a method of the model instance.  More precisely, it puts together a function based on the current state of the model - or on the state given as an argument to `logp` (see example below).\n",
    "\n",
    "For diverse reasons, we assume a `Model` instance isn't static. If you need to use `logp` in an inner loop and it needs to be static, simply use something like `logp = model.logp`. Here is an example below - note the caching effect and the speedup."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%timeit model.logp({mu: 0.1})\n",
    "logp = model.logp\n",
    "%timeit logp({mu: 0.1})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Probability distributions\n",
    "\n",
    "Every probabilistic program consists of observed and unobserved Random Variables (RVs). Observed RVs are defined via likelihood distributions, while unobserved RVs are defined via prior distributions. In PyMC3, probability distributions are available from the main module space."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "help(pm.Normal)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dir(pm.distributions.mixture)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Unobserved Random Variables\n",
    "\n",
    "Every unobserved RV has the following calling signature: name(str), parameter keyword arguments. Thus, a normal prior can be defined in a model context like this:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pm.Model():\n",
    "    x = pm.Normal('x', mu=0, sigma=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As with the model, we can evaluate its logp:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x.logp({'x': 0})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Observed Random Variables\n",
    "\n",
    "Observed RVs are defined just like unobserved RVs but require data to be passed into the observed keyword argument:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pm.Model():\n",
    "    obs = pm.Normal('obs', mu=0, sigma=1, observed=np.random.randn(100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "obs.logp({'mu': 0})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `observed` keyword supports values of type `list`, `numpy.ndarray`, `theano`, and `pandas` data structures."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Deterministic transforms\n",
    "\n",
    "PyMC3 allows you to freely do algebra in all kinds of ways:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pm.Model():\n",
    "    x = pm.Normal('x', mu=0, sigma=1)\n",
    "    y = pm.Gamma('y', alpha=1, beta=1)\n",
    "    plus_2 = x + 2\n",
    "    summed = x + y\n",
    "    squared = x ** 2\n",
    "    sined = pm.math.sin(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "While these transforms work seamlessly, their results are **not** stored automatically. Thus, if you want to keep track of a transformed variable, you must use `pm.Deterministic`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pm.Model():\n",
    "    x = pm.Normal('x', mu=0, sigma=1)\n",
    "    plus_2 = pm.Deterministic('plus_2', x + 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that `plus_2` can be used in identical to the above, we only tell PyMC3 to keep track of this RV for us."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Automatic transform of bounded RVs\n",
    "\n",
    "In order to sample models more efficiently, PyMC3 automatically transforms bounded RVs to be **unbounded**."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    x = pm.Uniform('x', lower=0, upper=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When we look at the RVs of the model, we would expect to find `x` there; however:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model.free_RVs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The variable, `x_interval__`, represents `x` transformed to accept parameter values between `-inf` and `+inf`. In the case of an upper and lower bound, a `LogOdds` transform is applied. Sampling in this transformed space makes it easier for the sampler. PyMC3 also keeps track of the non-transformed bounded parameters. These are common deterministics (see above):"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.deterministics"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When displaying results, PyMC3 will usually hide transformed parameters. You can pass the `include_transformed=True` parameter to many functions to see the transformed parameters that are used for sampling.\n",
    "\n",
    "You can also turn transforms off:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    x = pm.Uniform('x', lower=0, upper=1, transform=None)\n",
    "\n",
    "print(model.free_RVs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or specify different transformations other than the default:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pymc3.distributions.transforms as tr\n",
    "\n",
    "with pm.Model() as model:\n",
    "    # Use the default log transform\n",
    "    x1 = pm.Gamma('x1', alpha=1, beta=1)\n",
    "    # Specify a different transformation\n",
    "    x2 = pm.Gamma('x2', alpha=1, beta=1, transform=tr.log_exp_m1)\n",
    "\n",
    "print(f'The default transformation of x1 is: {x1.transformation.name}')\n",
    "print(f'The user-specified transformation of x2 is: {x2.transformation.name}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transformed distributions and changes of variables\n",
    "\n",
    "PyMC3 does **not** provide explicit functionality to transform one distribution to another. Instead, a dedicated distribution is usually created in consideration of optimising performance. However, users can still create transformed distribution by passing the inverse transformation to `transform` `kwarg`. Take the classic textbook example of `LogNormal`: $\\mathcal log(y) \\sim \\mathrm Normal(\\mu, \\sigma)$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Exp(tr.ElemwiseTransform):\n",
    "    name = 'exp'\n",
    "\n",
    "    def backward(self, x):\n",
    "        return tt.log(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return tt.exp(x)\n",
    "\n",
    "    def jacobian_det(self, x):\n",
    "        return -tt.log(x)\n",
    "\n",
    "with pm.Model() as model:\n",
    "    x1 = pm.Normal('x1', 0.0, 1.0, transform=Exp())\n",
    "    x2 = pm.Lognormal('x2', 0.0, 1.0)\n",
    "\n",
    "lognormal = model.named_vars['x1_exp__']\n",
    "lognorm2 = model.named_vars['x2']\n",
    "\n",
    "_, ax = plt.subplots(figsize=(5, 3))\n",
    "x = np.linspace(0.0, 10.0, 100)\n",
    "ax.plot(\n",
    "    x,\n",
    "    np.exp(lognormal.distribution.logp(x).eval()),\n",
    "    '--',\n",
    "    alpha=0.5,\n",
    "    label='log(y) ~ Normal(0, 1)',\n",
    ")\n",
    "ax.plot(\n",
    "    x,\n",
    "    np.exp(lognorm2.distribution.logp(x).eval()),\n",
    "    alpha=0.5,\n",
    "    label='y ~ lognormal(0, 1)',\n",
    ")\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice from above that the named variable `x1_exp__` in the `model` is Lognormal distributed."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using a similar approach, we can create ordered RVs following some distribution. For example, we can combine the ordered transformation and logodds transformation using `Chain` to create a 2D RV that satisfy $\\mathcal x1, x2 \\sim \\mathrm Uniform(0, 1) \\mathcal\\ and\\ x1 < x2$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Order = tr.Ordered()\n",
    "Logodd = tr.LogOdds()\n",
    "chain_tran = tr.Chain([Logodd, Order])\n",
    "\n",
    "with pm.Model() as m0:\n",
    "    x = pm.Uniform('x', 0.0, 1.0, shape=2, transform=chain_tran, testval=[0.1, 0.9])\n",
    "    trace = pm.sample(5000, tune=1000, progressbar=False, return_inferencedata=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "for ivar, varname in enumerate(trace.varnames):\n",
    "    ax[ivar].scatter(trace[varname][:, 0], trace[varname][:, 1], alpha=0.01)\n",
    "    ax[ivar].set_xlabel(f'{varname}[0]')\n",
    "    ax[ivar].set_ylabel(f'{varname}[1]')\n",
    "    ax[ivar].set_title(varname)\n",
    "plt.tight_layout()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### List of RVs / higher-dimensional RVs\n",
    "\n",
    "Above we have seen how to create scalar RVs. In many models, you want multiple RVs. There is a tendency (mainly inherited from PyMC 2.x) to create a list of RVs, like:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pm.Model():\n",
    "    # bad\n",
    "    x = [pm.Normal(f'x_[{i}]', mu=0, sigma=1) for i in range(10)]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, even though this works, it is quite slow and not recommended. Instead, use the `shape` kwarg."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    # good\n",
    "    x = pm.Normal('x', mu=0, sigma=1, shape=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "`x` is now a random vector of length 10. We can index into it or do linear algebra operations on it."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with model:\n",
    "    y = x[0] * x[1]  # full indexing is supported\n",
    "    x.dot(x.T)  # linear algebra is supported"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialization with test values\n",
    "\n",
    "While PyMC3 tries to automatically initialize models, it is sometimes helpful to define initial values for RVs. This can be done via the `testval` kwarg:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pm.Model():\n",
    "    x = pm.Normal('x', mu=0, sigma=1, shape=5)\n",
    "\n",
    "x.tag.test_value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pm.Model():\n",
    "    x = pm.Normal('x', mu=0, sigma=1, shape=5, testval=np.random.randn(5))\n",
    "\n",
    "x.tag.test_value"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This technique is quite useful to identify problems with model specification or initialization."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inference\n",
    "\n",
    "Once we have defined our model, we have to perform inference to approximate the posterior distribution. PyMC3 supports two broad classes of inference: sampling and variational inference."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sampling\n",
    "\n",
    "The main entry point to MCMC sampling algorithms is via the `pm.sample()` function. By default, this function tries to auto-assign the right sampler(s) and auto-initialize if you don't pass anything.\n",
    "\n",
    "With PyMC3 version >= 3.9, the `return_inferencedata=True` keyword argument makes the sample function return an `arviz.InferenceData` object instead of a `MultiTrace`. `InferenceData` has many advantages compared to a `MultiTrace`. For example, it can be saved to / loaded from a file, and it can also carry additional (meta)data such as date/version or posterior predictive distributions. Take a look at the [ArviZ Quickstart](https://arviz-devs.github.io/arviz/getting_started/Introduction.html) to learn more."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    mu = pm.Normal('mu', mu=0, sigma=1)\n",
    "    obs = pm.Normal('obs', mu=mu, sigma=1, observed=np.random.randn(100))\n",
    "\n",
    "    idata = pm.sample(2000, tune=1500, return_inferencedata=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, on a continuous model, PyMC3 assigns the NUTS sampler, which is very efficient even for complex models. PyMC3 also runs tuning to find good starting parameters for the sampler. Here we draw 2000 samples from the posterior in each chain and allow the sampler to adjust its parameters in an additional 1500 iterations. If not set via the `cores` keyword arg, the number of chains is determined from the number of available CPU cores."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idata.posterior.dims"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The tuning samples are discarded by default. With `discard_tuned_samples=False`, they can be kept and end up in a special property of the `InferenceData` object.\n",
    "\n",
    "You can also run multiple chains in parallel using the `chains` and `cores` keyword args."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    mu = pm.Normal('mu', mu=0, sigma=1)\n",
    "    obs = pm.Normal('obs', mu=mu, sigma=1, observed=np.random.randn(100))\n",
    "\n",
    "    idata = pm.sample(cores=4, chains=6, return_inferencedata=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idata.posterior['mu'].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get values of a single chain\n",
    "idata.posterior['mu'].sel(chain=1).shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "PyMC3 offers a variety of other samplers, found in `pm.step_methods`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list(filter(lambda n: n[0].upper(), dir(pm.step_methods)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Commonly used methods step-methods, besides NUTS are `Metropolis` and `Slice`. **For almost all continuous models, \"NUTS\" should be preferred**. There are hard-to-sample models for which NUTS will be very slow causing many users to use `Metropolis` instead. This practice, however, is rarely successful. NUTS is very fast on simple models but can be slow if the model is very complex or is badly initialized. In the case of a complex model that is hard for NUTS, Metropolis, while faster, will have a very low effective sample size or not converge properly at all. A better approach is to instead try to improve initialization of NUTS, or reparameterize the model.\n",
    "\n",
    "For completeness, other sampling methods can be passed to sample:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    mu = pm.Normal('mu', mu=0, sigma=1)\n",
    "    obs = pm.Normal('obs', mu=mu, sigma=1, observed=np.random.randn(100))\n",
    "\n",
    "    step = pm.Metropolis()\n",
    "    trace = pm.sample(1000, step=step)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can also assign variables to different step methods."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    mu = pm.Normal('mu', mu=0, sigma=1)\n",
    "    sd = pm. HalfNormal('sd', sigma=1)\n",
    "    obs = pm.Normal('obs', mu=mu, sigma=sd, observed=np.random.randn(100))\n",
    "\n",
    "    step1 = pm.Metropolis(vars=[mu])\n",
    "    step2 = pm.Slice(vars=[sd])\n",
    "    idata = pm.sample(10000, step=[step1, step2], cores=4, return_inferencedata=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analyze sampling results\n",
    "\n",
    "The most common used plot to analyze sampling results is the so-called trace-plot:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.plot_trace(idata)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another common tactic is to look at R-hat, also known as the Gelman-Rubin statistic:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.summary(idata)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "These are also part of the `forestplot`:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.plot_forest(idata, r_hat=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, for a plot of the posterior that is inspired by the book, [Doing Bayesian Data Analysis](http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/), you can use:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "az.plot_posterior(idata)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For high-dimensional models it becomes cumbersome to look at all the parameter's traces. When using NUTS we can look at the energy plot to assess problems of convergence:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    x = pm.Normal('x', mu=0, sigma=1, shape=100)\n",
    "    idata = pm.sample(cores=4, return_inferencedata=True)\n",
    "\n",
    "az.plot_energy(idata)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For more information on stats and the energy plot, see [here](https://docs.pymc.io/en/v3/pymc-examples/examples/diagnostics_and_criticism/sampler-stats.html). For more information on identifying sampling problems and what to do about them, see [here](https://docs.pymc.io/en/v3/pymc-examples/examples/diagnostics_and_criticism/Diagnosing_biased_Inference_with_Divergences.html)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Variational inference\n",
    "\n",
    "PyMC3 supports various Variational Inference techniques. While these methods are much faster, they are often also less accurate and can lead to biased inference. The main entry point is `pymc3.fit()`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    mu = pm.Normal('mu', mu=0, sigma=1)\n",
    "    sd = pm.HalfNormal('sd', sigma=1)\n",
    "    obs = pm.Normal('obs', mu=mu, sigma=sd, observed=np.random.randn(100))\n",
    "\n",
    "    approx = pm.fit()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The returned approximation object has various capabilities, like drawing samples from the approximated posterior, which we can analyze like a regular sampling run:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "approx.sample(500)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The variational submodule offers a lot of flexibility in which VI uses and follows an object-oriented design. For example, full rank ADVI estimates a full covariance matrix:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mu = pm.floatX([0.0, 0.0])\n",
    "cov = pm.floatX([[1, 0.5], [0.5, 1.0]])\n",
    "with pm.Model() as model:\n",
    "    pm.MvNormal('x', mu=mu, cov=cov, shape=2)\n",
    "    approx = pm.fit(method='fullrank_advi')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "An equivalent expression using the object-oriented interface is:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    pm.MvNormal('x', mu=mu, cov=cov, shape=2)\n",
    "    approx = pm.FullRankADVI().fit()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "trace = approx.sample(10000)\n",
    "az.plot_kde(trace['x'][:, 0], trace['x'][:, 1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Stein Variational Gradient Descent (SVGD) uses particles to estimate the posterior:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w = pm.floatX([0.2, 0.8])\n",
    "mu = pm.floatX([-0.3, 0.5])\n",
    "sd = pm.floatX([0.1, 0.1])\n",
    "with pm.Model() as model:\n",
    "    pm.NormalMixture('x', w=w, mu=mu, sigma=sd)\n",
    "    approx = pm.fit(method=pm.SVGD(n_particles=200, jitter=1.0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "trace = approx.sample(10000)\n",
    "az.plot_dist(trace['x'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For more information on variational inference, see [these examples](http://pymc-devs.github.io/pymc3/examples.html#variational-inference)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Posterior Predictive Sampling\n",
    "\n",
    "The `sample_posterior_predictive()` function performs predictions on hold-out data and posterior predictive checks."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = np.random.randn(100)\n",
    "with pm.Model() as model:\n",
    "    mu = pm.Normal('mu', mu=0, sigma=1)\n",
    "    sd = pm.HalfNormal('sd', sigma=1)\n",
    "    obs = pm.Normal('obs', mu=mu, sigma=sd, observed=data)\n",
    "\n",
    "    idata = pm.sample(return_inferencedata=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with model:\n",
    "    post_pred = pm.sample_posterior_predictive(idata.posterior)\n",
    "\n",
    "az.concat(idata, az.from_pymc3(posterior_predictive=post_pred), inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "az.plot_ppc(idata, ax=ax)\n",
    "ax.axvline(data.mean(), ls='--', color='r', label='True mean')\n",
    "ax.legend(fontsize=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predicting on hold-out data\n",
    "\n",
    "In many cases you want to predict on unseen / hold-out data. This is especially relevant in Probabilistic Machine Learning. We recently improved the API in this regard with the `pm.Data` container. It is a wrapper around a `theano.shared` variable whose values can be changed later. Otherwise, they can be passed into PyMC3 just like any other `numpy` array or tensor.\n",
    "\n",
    "This distinction is significant since internally all models in PyMC3 are giant symbolic expressions. When you pass data directly into a model, you are giving Theano permission to treat this data as a constant and optimize it away as it sees fit. If you need to change this data later you might not have a way to point at it in the symbolic expression. Using `theano.shared` offers a way to point to a place in that symbolic expression, and change what is there."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = np.random.randn(100)\n",
    "y = x > 0\n",
    "\n",
    "with pm.Model() as model:\n",
    "    # Create a shared variable that can be changed later on\n",
    "    x_shared = pm.Data('x_obs', x)\n",
    "    y_shared = pm.Data('y_obs', y)\n",
    "\n",
    "    coeff = pm.Normal('x', mu=0, sigma=1)\n",
    "    logistic = pm.math.sigmoid(coeff * x_shared)\n",
    "    pm.Bernoulli('obs', p=logistic, observed=y_shared)\n",
    "    idata = pm.sample(return_inferencedata=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now assume we want to predict on unseen data. For this we have to change the values of `x_shared` and `y_shared`. Theoretically we don't neet to set `y_shared` as we want to predict it, but it has to match the shape of `x_shared`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with model:\n",
    "    # Change the value and shape of the data\n",
    "    pm.set_data(\n",
    "        {\n",
    "            'x_obs': [-1, 0, 1.0],\n",
    "            # Use dummy values with the same shape:\n",
    "            'y_obs': [0, 0, 0],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    post_pred = pm.sample_posterior_predictive(idata.posterior)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "post_pred['obs'].mean(axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}